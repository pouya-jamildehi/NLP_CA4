{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWqO0hF_lN-E"
      },
      "source": [
        "# NLP CA4: `Hands-On`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkttvzyvi5Y2"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip install -q accelerate peft bitsandbytes transformers trl sentencepiece triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWDTZNa84AKO"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpuuyuKd3_T5"
      },
      "source": [
        "Create a Huggingface Access Token From:\n",
        "https://huggingface.co/settings/tokens\n",
        "\n",
        "You need to request for access to:\n",
        "- ```google/gemma-2-2b-it```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wkXr6FRvpcA"
      },
      "outputs": [],
      "source": [
        "# HF_TOKEN = 'XXX'\n",
        "!hf auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ8yx1dmNt0O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = \"cpu\"\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIByh2PKlnph"
      },
      "source": [
        "## Loading the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwLiVXWz01cP"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"google/gemma-2-2b\"\n",
        "INSTRUCT_MODEL = \"google/gemma-2-2b-it\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slFjVBsMW_b6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_id = INSTRUCT_MODEL\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=DEVICE,\n",
        "    dtype=\"float16\",\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPOQsuMtNt0O"
      },
      "source": [
        "Tokenization inspection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AfQ_dJcNt0P"
      },
      "outputs": [],
      "source": [
        "text = \"Hello world! من یک دانشجو هستم.\"\n",
        "\n",
        "encoded = tokenizer(text, return_tensors=\"pt\")\n",
        "print(\"Token IDs:\", encoded[\"input_ids\"][0])\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0]))\n",
        "print(\"Number of tokens:\", len(encoded[\"input_ids\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0BHclV3L5zP"
      },
      "source": [
        "Sea the structure of LLMs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnAkBc_wmbLQ"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjufE6Sg_2xN"
      },
      "source": [
        "## Inference model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjGNyKIhNt0P"
      },
      "source": [
        "The outputs of tokenizer are not human readable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnmFo9_CNt0P"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a sentence with five words.\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "generation_output = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"],\n",
        ")\n",
        "\n",
        "generation_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-sgHPvf63Ke"
      },
      "outputs": [],
      "source": [
        "# Decode the output\n",
        "print(tokenizer.decode(generation_output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3huBhUaNt0Q"
      },
      "source": [
        "### Base Model vs. Instruction-tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG7slOnbNt0Q"
      },
      "source": [
        "See the difference between Base and Instruct Models using the prompt ```What is 2+2?```, Keep in mind that when temperature != 0, you will get different answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPsJSuLZNt0Q"
      },
      "outputs": [],
      "source": [
        "# WRITE YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXVKukJ5Akpc"
      },
      "source": [
        "### Speeding up generation by caching keys and values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lPss3e2_8Hi"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "# Tokenize the input prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRajhveqNt0Q"
      },
      "source": [
        "**Compare runtime:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOHdAulN_7z9"
      },
      "outputs": [],
      "source": [
        "%%timeit -n 1\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG6xBKMZBxks"
      },
      "outputs": [],
      "source": [
        "%%timeit -n 1\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-iKxENt2vdC"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.chat_template)"
      ],
      "metadata": {
        "id": "B2gt24TbS84l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWI4r_3gQEDI"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")\n",
        "\n",
        "# Prompt\n",
        "messages = [\n",
        "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeiQle30VTGJ"
      },
      "outputs": [],
      "source": [
        "# Greedy output generation\n",
        "output = pipe(messages, do_sample=False)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxDcY9eZMVFE"
      },
      "source": [
        "**Compare different temperature:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwaRcUqfVXIj"
      },
      "outputs": [],
      "source": [
        "# Using a high temperature\n",
        "output = pipe(messages, do_sample=True, temperature=1)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cBfdZ0ZMZ-G"
      },
      "source": [
        "**Compare different top_p:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "536X6zlBV4BC"
      },
      "outputs": [],
      "source": [
        "# Using a high top_p\n",
        "output = pipe(messages, do_sample=True, top_p=1)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "beUvboIJVEQ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}